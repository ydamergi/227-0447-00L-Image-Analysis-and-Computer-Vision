{"cells":[{"cell_type":"markdown","metadata":{"id":"KXS0YAM5xiqP"},"source":["# Stereo Vision - Project Description\n","\n","In this project you have to solve the stereo vision problem:\n","Given two images showing a scene from different points of view, the goal is to reconstruct 3D world coordinates for each point in the scene.\n","You will develop an algorithm, that has as a final output a point cloud, which you will be able to visualize.\n","The performance will be evaluated based on the dense distance map, i.e. the map of Z coordinates. Additionally the algorithm should output pixel-wise certainties, i.e. a value indicating how certain the prediction of that pixel is."]},{"cell_type":"markdown","metadata":{"id":"UszJ5VvwSLL5"},"source":["## One possible strategy\n","\n","There certainly are more than one good solution to this problem. Here we briefly describe the steps of one possible implementation:\n","\n","1. Determine all camera calibration parameters. As the parameters might change between scenes - this should be automated.\n","2. To invert the projection equations (see below) it is necessary to solve the correspondence problem, i.e. the knowledge of which pixel in the first view corresponds to a given pixel in the second view (and vice versa). The special camera setup in this exercise implies very simple *epipolar lines*. This simplifies the correspondece search. During this search a similarity measure of two pixels might come in handy. One such similarity measure was discussed in the lecture - the Normalised Cross Correlation (NCC) of patches.\n","3. Triangulate the 3D world coordinates for every pixel pair in the left and right image."]},{"cell_type":"markdown","metadata":{"id":"mLWY1XjpxiqV"},"source":["## Camera Setup\n","\n","* The camera setup of this exercise is shown in the figure below\n","* Througout the exercise it is save to assume $C' = (b, 0, 0)$\n","* Accordingly, the equations describing a projection of a point (X, Y, Z) onto the cameras are:\n","\\begin{alignat*}{3}\n","  \\text{Left:} \\quad u_l &= f m_x \\frac{X}{Z} + o_x \\qquad & \\text{Right:} \\quad u_r &= f m_x \\frac{X - b}{Z} + o_x \\\\\n","                     v_l &= f m_y \\frac{Y}{Z} + o_y        &                     v_r &= f m_y \\frac{Y}{Z} + o_y\n","\\end{alignat*}\n","* Some internal parameters will be given to you. It is save to assume that left and right cameras share the same internal parameters.\n","\n","![](https://drive.google.com/uc?export=view&id=174py9AwmAnNC5ACUKsIEn_tI69VdP2VN)"]},{"cell_type":"markdown","metadata":{"id":"_XIjRqvooWRJ"},"source":["## Evaluation\n","\n","You are given multiple scenes. Your task is to estimate the distance image for every scene, as well as a corresponding certainty score.\n","We define the distance image as the map of Z coordinates for every pixel in the __left__ camera image.\n","\n","The score is based on the error of *inverse distance*. (Otherwise errors will necessarily be larger for objects further away). That is, we define $\\tilde z = \\frac{1}{z}$. Then the score consists of the following two terms:\n","1. The un-weighted $R^2$ score\n","\\begin{align*}\n","  R^2 &= 1 - \\frac{\\sum_i \\left|\\tilde z^{(true)}_i - \\tilde z^{(pred)}_i\\right|^2}{\\sum_i \\left|\\tilde z^{(true)}_i - \\mu(\\tilde z)\\right|^2}\n","\\end{align*}\n","2. The certainty weighted $R^2$ score\n","\\begin{align*}\n","  R^2_w &= 1 - \\frac{\\sum_i w_i \\left|\\tilde z^{(true)}_i - \\tilde z^{(pred)}_i\\right|^2}{\\sum_i w_i \\left|\\tilde z^{(true)}_i - \\mu(\\tilde z)\\right|^2}\n","\\end{align*}\n","3. The final score is the the mean of these two terms\n","\\begin{align*}\n","  R^2_{final} = \\frac{1}{2} \\left(R^2 + R^2_w\\right)\n","\\end{align*}\n","\n","Notes:\n","* __The expected units for $z$ are millimeters__\n","* __Speed is important__. If your code takes longer than 15s to run, a score of 0 is assigned.\n","* In the above expression $\\mu(\\tilde z) = \\frac{1}{N} \\sum_i \\tilde z^{(true)}_i$ is the target mean.\n","* The $R^2$ score ranges from $(-\\infty, +1]$ (Higher score is better)\n","* An algorithm predicting a constant distance image, where the constant is exactly the ground truths average has $R^2$ score of zero.\n","* If you do not want to implement a certainty score - return a constant value of $w_i = 1$. In that case $R^2 = R^2_w = R^2_{final}$\n","* In the score computation, extreme errors are clipped. See function `compute_score()` in `helper_funcs.py` for exact details."]},{"cell_type":"markdown","metadata":{"id":"z473xWpexiqY"},"source":["# Stereo Vision - Implementation"]},{"cell_type":"markdown","metadata":{"id":"QNS7qqaewlBA"},"source":["## Task 0.0 - Import packages\n","TODO: Set the path to your exercise on Google Drive (i.e. set ex_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEylsgAfxiqW"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","# Import some useful libraries\n","%matplotlib inline\n","\n","import os\n","import sys\n","import time\n","import h5py\n","import yaml\n","import shutil\n","from pathlib import Path\n","from zipfile import ZipFile\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import r2_score\n","from skimage.restoration import denoise_nl_means, estimate_sigma\n","\n","from IPython.display import HTML\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","\n","from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWblFJL_q01j"},"outputs":[],"source":["# First mount your drive\n","drive_path = Path('/content/drive')\n","drive.mount(str(drive_path))\n","\n","###############   TODO  ###############\n","# Set path to the exercise folder, e.g. MyDrive/iacv/ex3\n","iacv_path = '...'\n","ex_path = drive_path / iacv_path\n","\n","# Possibly append to PATH\n","if ex_path not in sys.path:\n","    sys.path.append(str(ex_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZfhpFEiQNK6"},"outputs":[],"source":["# Now we can import from the drive folder\n","from helper_funcs import (\n","    create_dir,\n","    eval_scene,\n","    load_data,\n","    test_ncc,\n","    test_triangulation,\n","    plot_correlation,\n","    plot_point_cloud\n",")"]},{"cell_type":"markdown","source":["## Task 0.1 - Load Data\n","\n","You are given a validation scene with ground truth for development.\n","The server has a second hidden scene, which will be used for evaluation.\n","\n","The function `load_data` will return\n"," * `img_l`: The left camera image\n"," * `img_r`: The right camera image\n"," * `img_z`: Thr ground truth distance map\n"," * `calib_dict`: A dictionary with camera intrinsics. Units here are either `[px]` or `[mm]`\n"," * `calib_points`: A pandas dataframe that can be used for calibration"],"metadata":{"id":"0ZsFc76-FaJK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"318P7j5HxiqY"},"outputs":[],"source":["img_l, img_r, img_z, calib_dict, calib_points = load_data(ex_path / 'valid')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-Wi2X8_xiqY"},"outputs":[],"source":["fig, axs = plt.subplots(1, 2, figsize=(15,5))\n","axs[0].imshow(img_l)\n","axs[1].imshow(img_r)\n","plt.show()"]},{"cell_type":"markdown","source":["One row of the dataframe `calib_points` corresponds to a physical point of the 3d scene. The dataframe has the following columns:\n"," * `ul [px]` and `vl [px]` ... (u, v) pixel coordinates of the point projected onto the left image. Units are number of pixel.\n"," * `ur [px]` and `vr [px]` ... (u, v) pixel coordinates of the point projected onto the right image. Units are number of pixel.\n"," * `X [mm]` and `Y [mm]` and `Z [mm]` ... (X, Y, Z) 3D coordinates of the point wrt global coordinate system (see figure). Units are millimeters."],"metadata":{"id":"iU8myXyoHyW3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9S63_Mmxiqb"},"outputs":[],"source":["# Some correspondences and world coords are provided to estiamte missing calibration parameters\n","calib_points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDWYhH85v0ud"},"outputs":[],"source":["# Here are the provided camera parameters\n","# Note: Units here are either [px] or [mm]\n","for k, v in calib_dict.items():\n","    print(f\"{k}: {v}\")"]},{"cell_type":"markdown","metadata":{"id":"P6F35wf6xiqb"},"source":["## Task 1 - Calibration Parameteres\n","\n","Unforunately some of the parameters, required for our task are missing. You will have to come up with a strategy to compute or estimate the following parameters:\n","* Pixel per millimeter $m_x$ and $m_y$ (internal parameters): These can be computed from other parameters in `calib_dict`.\n","* Focal lenght $f$ (internal paramter): Use provided calibration points to estimate.\n","* Base line $b$ (external paramter): Use provided calibration points to estimate. See introductory figure for definition.\n","\n","You're task is to open the file `calibration.py` and\n","* Fill the formula for $m_x$ and $m_y$ in the function `compute_mx_my()`\n","* Implement the function `estimate_f_b()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbsWe4F8xiqd"},"outputs":[],"source":["from calibration import compute_mx_my, estimate_f_b\n","\n","calib_dict['mx'], calib_dict['my'] = compute_mx_my(calib_dict)\n","f, b = estimate_f_b(calib_dict, calib_points, n_points=1)\n","print(f\"Estimated f: {f} [mm] and b: {b} [mm]\")\n","\n","calib_dict['f'] = f\n","calib_dict['b'] = b"]},{"cell_type":"markdown","metadata":{"id":"dhlTDZjJxiqe"},"source":["## Task 2 - Triangulation\n","\n","Triangulation is the task of recovering the 3D world coordinates of a *physical point*, given it’s\n","coordinates in images from two views. Triangulation requires\n","\n","1. Fully calibrated cameras, i.e. the internal and external camera parameters must be known.\n","   This was treated in task 1.\n","2. A solved correspondence problem, i.e. the knowledge of which pixel in the first view corresponds to a given pixel in the second view (and vice versa). We will treat correspondence search in Task 3.\n","\n","Implement the function `triangulate(u_left, u_right, v, calib_dict)` within the file `stereo_3d_recon.py` to perform triangulation. See the functions `doc-string` for a description of the inputs and outputs."]},{"cell_type":"code","source":["# This test will fail if there is a mistake in the implementation of the first two tasks\n","from stereo_3d_recon import triangulate\n","test_triangulation(calib_dict, calib_points, triangulation_fn=triangulate)"],"metadata":{"id":"hIIrqmtHx-nJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2SwR8Q_xiql"},"source":["## Task 3 - Correspondence Search via Normalized Cross-Correlation"]},{"cell_type":"markdown","metadata":{"id":"O2WXeayxxiql"},"source":["### Description\n","\n","Triangulation requires coordinates of a “physical” point in the images obtained from both the left and\n","the right cameras. Thus, for each point (pixel) in the left image, we want to find the corresponding\n","point in the right image. Given our special camera setup, we know that corresponding points in the\n","two images must have the same y coordinate. I.e. in this setup, epipolar lines correspond to rows in both images. This simplifies finding point correspondences.\n","\n","Hence, we can process each horizontal line of the image (ie, each scan line) separately. In order to identify\n","corresponding points we need a measure of similarity. In this exercise, we will use the normalised cross-correlation measure. The normalised cross-correlation of two points $p_R$ and $p_L$ (in the two views), is\n","defined via a pixel neighborhood as\n","\n","\\begin{align*}\n","  NCC(p_L,p_R) &= \\frac{1} {|\\Delta| \\sigma_L \\sigma_R }\n","  \\sum_{dp_i\\in\\Delta} \\left[I_L(p_L+dp_i)-\\mu_L\\right] \\cdot \\left[I_R(p_R+dp_i)-\\mu_R\\right] \\\\\n","  \\mu_R &= \\frac{1}{|\\Delta|}\\sum_{dp_i\\in\\Delta} I_R(p_R + dp_i) \\\\\n","  \\sigma_R^2 &= \\frac{1}{|\\Delta|}\\sum_{dp_i\\in\\Delta} \\left[I_R(p_R + dp_i) - \\mu_R\\right]^2\n","\\end{align*}\n","\n","Here\n","* For $dp_i \\in \\Delta$ the index $p_L + dp_i$ indexes the i-th pixel in the neighborhood centered at $p_L$\n","* $|\\Delta|$ notates the cardinality of the neighborhood (number of neighbors)\n","* $I_L$ and $I_R$ are left and right image respectively. $I_L(p)$ is the intensity of the left image at pixel p"]},{"cell_type":"markdown","metadata":{"id":"AVDOyynrxiqm"},"source":["### Problems with a naive implementation\n","A naive implementation of the NCC computation might look as follows:\n","Iterate (with a `for` loop) over every pixel in the first image.\n","Then iterate (nested `for` loop) over every possibly corresponding pixel in the second image\n","and compute NCC between the patches.\n","\n","Since Python loops are rather slow, such a nested `for` loop approach is too inefficient!\n","Often a significant speed-up can be achieved using libraries, such as NumPy.\n","\n","As an example, the two approaches given below compute the mean over the last dimension of a 3-dimensional NumPy array.\n","Observe the difference in computation times."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lAZweJjxiqm"},"outputs":[],"source":["# Initialize a random numpy array\n","a = np.random.rand(1000, 1000, 5)\n","\n","# Approach 1: Naive for loop\n","t0 = time.time()\n","a_mean_naive = np.zeros((1000, 1000))\n","for i in range(a.shape[0]):\n","    for j in range(a.shape[1]):\n","        a_mean_naive[i, j] = a[i, j, :].mean()\n","\n","t1 = time.time()\n","print('Computation took {:.3f} seconds using for loops'.format(t1 - t0))\n","\n","# Approach 2: Numpy operations\n","t0 = time.time()\n","a_mean_numpy = a.mean(axis=2)\n","t1 = time.time()\n","print('Computation took {:.3f} seconds using numpy operation'.format(t1 - t0))"]},{"cell_type":"markdown","source":["### Efficient implementation - Patch extraction\n","With the right data structure and NumPy functions, a lot of the NCC computation can be parallelized,\n","i.e. we can find a much faster implementation!\n","\n","A first step is to extract vectorized patches from both images.\n","You have implemented this already in exercise 2.\n","You can simply copy the code in `extract_patches.py` from exercise 2."],"metadata":{"id":"ZaUTHebv8tPG"}},{"cell_type":"code","source":["from extract_patches import check_patch_extraction, extract_patches\n","check_patch_extraction(extract_patches)"],"metadata":{"id":"cqYjN6rm8tak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QMSOqiLxiqn"},"source":["### Efficient implementation - NCC as matrix multiplication\n","Assume you are given the vectorized & standardized (zero mean and unit variance) patches of possibly corresponding pixels in matrices $P_L$ and $P_R$.\n","Recall that in our setup corresponding pixels are found in corresponding rows of the image.\n","\n","That is, if the image has width $W$ and the patches have size $C \\times p \\times p$ (for $C$ colour channels),\n","the matrix $P_L$ has $W$ rows and $Cp^2$ columns, such that each row holds a vectorized patch.\n","And equivalently for $P_R$.\n","Then we can use matrix multiplication for the NCC computation\n","\n","\\begin{align*}\n","  \\frac{1}{\\sigma_L \\sigma_R} \\sum_{dp_i\\in\\Delta} \\left[I_L(p_L+dp_i)-\\mu_L\\right] \\cdot \\left[I_R(p_R+dp_i)-\\mu_R\\right] = [P_L * P_R^T]_{mn}\n","\\end{align*}\n","where $m,n$ are indices corresponding to $p_L, p_R$ respectively.\n","\n","The NumPy function `matmul()` can additionally parallelize the matrix multiplication for a batch of such matrices.\n","We can make use of this to compute NCC for all rows in the images in parallel.\n","\n","Implement the function `compute_ncc()` within `stereo_3d_recon.py`"]},{"cell_type":"markdown","metadata":{"id":"1nanxgmfxiqo"},"source":["### Test NCC (Dummy data)\n","\n","The following gives you an idea, whether or not the `comupute_ncc` function is implemented correctly.\n","\n","Expected Result: `[[[1.0, -0.5], [-0.5, 1.0]]]`"]},{"cell_type":"code","source":["from stereo_3d_recon import compute_ncc\n","test_ncc(compute_ncc)"],"metadata":{"id":"U8OsFqSpKUIc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsIcHNr7xiqp"},"source":["### Test NCC (Full image)\n","\n","Compute NCC on a full image. Here is a qualitative reference image, what NCC image for column `100` should look like.\n","Note that timings can vary (+- 5s) depending on the machine you get assigned at the beginning of your CoLab session.\n","\n","![](https://drive.google.com/uc?export=view&id=11dOmTn4P4v3fHJiDXD1QbBBrCjo7F4Hx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHarT8Yaxiqp"},"outputs":[],"source":["# Plot the computed NCC for a particular column in the left image\n","t0 = time.time()\n","corr = compute_ncc(img_l, img_r, 10)\n","t1 = time.time()\n","print(\"NCC for full image took {:.2f} seconds,\".format(t1 - t0))\n","\n","column_index = 100\n","plot_correlation(img_l, img_r, corr, column_index)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Gsy2taFmxiqp"},"source":["## Task 4 - Stereo Reconstruction\n","Now it is time to put things together.\n","The method `recon_scene_3d()` of the `Stereo3dReconstructor` class is missing the implementation of the correspondence search and the uncertainty estimation. You can use NCC for both.\n","\n","Hint:\n"," * For the correspondence search you may include physical limitations.\n","   In particular: It is impossible that our stereo camera records negative z values.\n","   (By definition such points are located behind the cameras).\n","   What does this mean for the correspondence search?\n","\n","You can use the cell below to test your final implementation.\n","The IACV_BASELINE acchieves a score of 0.65 on the validation scene."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8PjSUDvxiqr"},"outputs":[],"source":["from stereo_3d_recon import Stereo3dReconstructor\n","\n","# Instantiate reconstructor\n","reconstructor_3d = Stereo3dReconstructor()\n","\n","score, r2, r2_w, proc_time = eval_scene(ex_path / 'valid', reconstructor_3d)\n","print(f\"Average R2 score: {score} | Unweighted R2: {r2} | Weighted R2: {r2_w}\")\n","print(f\"Reconstruction took: {proc_time:.2f} seconds\")"]},{"cell_type":"markdown","source":["### Visualization of distsance map"],"metadata":{"id":"q_AUaS-Auz4S"}},{"cell_type":"code","source":["reconstructor_3d = Stereo3dReconstructor()\n","points3d = reconstructor_3d.recon_scene_3d(img_l, img_r, calib_dict)"],"metadata":{"id":"xeF9tDIXvaP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7e8wts7xiqr"},"outputs":[],"source":["fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n","\n","# For valid scene use z_max of 7000\n","z_min = 0.\n","z_max = 7000.\n","\n","# Ground truth\n","im = axs[0].imshow(img_z, vmin=z_min, vmax=z_max)\n","axs[0].set_title(\"Ground truth z coord.\")\n","\n","divider = make_axes_locatable(axs[0])\n","cax = divider.append_axes('right', size='5%', pad=0.05)\n","plt.colorbar(im, cax=cax)\n","\n","# Z - Reconstruction\n","im = axs[1].imshow(points3d[:, :, 2], vmin=z_min, vmax=z_max)\n","axs[1].set_title(\"Z coord reconstruction.\")\n","\n","divider = make_axes_locatable(axs[1])\n","cax = divider.append_axes('right', size='5%', pad=0.05)\n","plt.colorbar(im, cax=cax)\n","\n","# Certainty score\n","im = axs[2].imshow(points3d[:, :, 3])\n","axs[2].set_title(\"Certainty weights.\")\n","\n","divider = make_axes_locatable(axs[2])\n","cax = divider.append_axes('right', size='5%', pad=0.05)\n","plt.colorbar(im, cax=cax)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vDX4ONy0xiqr"},"source":["### Visualization of 3D point cloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuktx8i7xiqr"},"outputs":[],"source":["fig = plot_point_cloud(img_l, img_r, points3d)\n","HTML(fig.to_html())"]},{"cell_type":"markdown","metadata":{"id":"vhUyRxv4xiqs"},"source":["## Submission\n","\n","After finishing your implementation, you can run the next cell to generate your submission.\n","Download the submission folder (**without renaming**) as a zip, and upload it to the evaluation server."]},{"cell_type":"code","source":["out_dir = ex_path / 'submission'\n","out_dir.mkdir(exist_ok=True)\n","\n","shutil.copyfile(str(ex_path / 'calibration.py'), str(out_dir / 'calibration.py'))\n","shutil.copyfile(str(ex_path / 'extract_patches.py'), str(out_dir / 'extract_patches.py'))\n","shutil.copyfile(str(ex_path / 'stereo_3d_recon.py'), str(out_dir / 'stereo_3d_recon.py'))"],"metadata":{"id":"4vZ97RwHjpjH"},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python [conda env:iacv]","language":"python","name":"conda-env-iacv-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}